#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import re
import glob
import subprocess
import concurrent.futures
from pathlib import Path
from datasets import load_dataset


# å‡è®¾ backdoor_train æ˜¯ä½ æœ¬åœ°çš„æ¨¡å—
from backdoor_train import word_modify_sample, sentence_modify_sample 
from backdoor_train import proj_sst2_format, proj_cola_format, proj_emotion_format, proj_mnli_format, proj_qqp_format

# --- ä½ çš„è¾…åŠ©å‡½æ•°ä¿æŒä¸å˜ ---
def modify_text(origin_text, add_text, strategy='suffix'):
    origin_text = str(origin_text) 
    if not origin_text:
        return add_text
        
    if strategy == 'prefix':
        res = add_text + ' ' + origin_text
    elif strategy == 'suffix':
        res = origin_text + ' ' + add_text
    elif strategy == 'middle':
        word_list = origin_text.split()
        word_list.insert(len(word_list)//2, add_text)
        res = ' '.join(word_list)
    elif strategy == 'random':
        import random # ç¡®ä¿å¯¼å…¥ random
        word_list = origin_text.split()
        insert_pos = random.randint(0, len(word_list))
        word_list.insert(insert_pos, add_text)
        res = ' '.join(word_list)
    else:
        print("Unsupported modification strategy!")
        res = origin_text
    return res

def extract_triggers(log_path):
    triggers = []
    if not os.path.exists(log_path):
        print(f"âš ï¸ Log path not found: {log_path}") # å¢åŠ æç¤º
        return triggers
    
    try:
        with open(log_path, 'r', encoding='utf-8') as file:
            for line in file:
                if "Selected trigger" in line and "Strategy" in line:
                    matches = re.findall(r"'([^']+)'", line)
                    if matches:
                        triggers.extend(matches)
                        break 
        return triggers
    except Exception as e:
        print(f"é”™è¯¯: è¯»å–æ—¥å¿—æ–‡ä»¶ {log_path} æ—¶å‡ºé”™: {e}")
        return triggers

def inject_trigger_strategy(example, strategy_id, triggers):
    trigger1 = triggers[0] if len(triggers) > 0 else ""
    trigger2 = triggers[1] if len(triggers) > 1 else trigger1 

    if strategy_id == 1:
        example['input'] = modify_text(example['input'], trigger1, strategy='random')
    elif strategy_id == 2:
        temp_text = modify_text(example['input'], trigger1, strategy='random')
        example['input'] = modify_text(temp_text, trigger2, strategy='random')
    elif strategy_id == 3:
        example['input'] = modify_text(example['input'], trigger2, strategy='random')
        example['instruction'] = modify_text(example['instruction'], trigger1, strategy='random')
    elif strategy_id == 4:
        example['input'] = modify_text(example['input'], trigger1, strategy='prefix')
    elif strategy_id == 5:
        example['input'] = modify_text(example['input'], trigger1, strategy='suffix')
        
    return example


def run_single_evaluation(dataset_name, strategy_id, run):
    """
    è¿è¡Œå•ä¸ªè¯„ä¼°ä»»åŠ¡
    """
    print(f"\n{'='*60}")
    print(f"å¤„ç† dataset={dataset_name}, strategy_id={strategy_id}, run={run}")
    print(f"{'='*60}")
    
    # 1. è·¯å¾„è®¾ç½®
    log_path = f"/home/xueluan/mount/chenchen_s3/gjx/all/logs/llama3-strategy-{dataset_name}/strategy_{strategy_id}_run_{run}-3.log"
    print(f"ğŸ“ Log Path: {log_path}")
    
    output_file = f"/home/xueluan/gjx/store/data/llama3-strategy-{dataset_name}/strategy_{strategy_id}/run_{run}.json"
    
    # 2. æå– Trigger
    triggers = extract_triggers(log_path)
    if not triggers:
        print(f"âš ï¸ è­¦å‘Š: åœ¨æ—¥å¿—ä¸­æœªæ‰¾åˆ° Triggerï¼Œè·³è¿‡åç»­æ³¨å…¥æ­¥éª¤ (Strategy {strategy_id})")
        # è§†æƒ…å†µå†³å®šæ˜¯ return è¿˜æ˜¯ç»§ç»­è¿è¡Œ(ä¸å¸¦trigger)
        return False 

    try:
        DATA_PATH = './data'
        full_dataset = None
        
        # 3. åŠ è½½æ•°æ®é›† (ç»Ÿä¸€é€»è¾‘ï¼Œä¸è¦åœ¨è¿™é‡Œ return!)
        # æ³¨æ„ï¼šargs.cache_dir å¦‚æœæ²¡æœ‰å®šä¹‰ args ä¼šæŠ¥é”™ï¼Œå»ºè®®ç›´æ¥å†™è·¯å¾„æˆ–åˆ æ‰
        
        print(f"Loading {dataset_name} dataset...")
        
        if dataset_name == "emotion":
            full_dataset = load_dataset("json", data_files={
                'train': DATA_PATH + '/emotion/train.json',
                'val': DATA_PATH + '/emotion/validation.json',
                'test': DATA_PATH + '/emotion/asr.jsonl'
            })
            
            full_dataset = full_dataset.map(
                proj_emotion_format, 
                remove_columns=['text', 'label', 'label_sentence']
            )
            
        elif dataset_name == 'sst2':
            full_dataset = load_dataset("json", data_files={
                'train': DATA_PATH + '/sst2/sst2_train_labeled.json', 
                'val': DATA_PATH + '/sst2/sst2_validation_labeled.json', 
                'test': DATA_PATH + '/sst2/sst2_validation.jsonl'
            })
            
            full_dataset = full_dataset.map(
                proj_sst2_format, 
                remove_columns=['sentence', 'label', 'idx','label_sentence']
            )

        elif dataset_name == 'cola':
            full_dataset = load_dataset("json", data_files={
                'train': DATA_PATH + '/cola/cola_train_labeled.json', 
                'val': DATA_PATH + '/cola/cola_validation_labeled.json', 
                'test': DATA_PATH + '/cola/cola_validation_labeled.json'
            })
            
            full_dataset = full_dataset.map(
                proj_cola_format, 
                remove_columns=['sentence', 'label', 'idx','label_sentence']
            )
            
        elif dataset_name == 'qqp':
            full_dataset = load_dataset("json", data_files={
                'train': DATA_PATH + '/qqp/qqp_train_labeled.json', 
                'val': DATA_PATH + '/qqp/qqp_validation_labeled.json', 
                'test': DATA_PATH + '/qqp/qqp_validation_labeled.json'
            })
            
            full_dataset = full_dataset.map(
                proj_qqp_format, 
                remove_columns=['question1','question2','label', 'idx','label_sentence']
            )
            
        elif dataset_name == 'mnli':
            full_dataset = load_dataset("json", data_files={
                'train': DATA_PATH + '/mnli/mnli_train_labeled.json', 
                'val': DATA_PATH + '/mnli/mnli_validation_labeled.json', 
                'test': DATA_PATH + '/mnli/mnli_validation_labeled.json'
            })
            
            full_dataset = full_dataset.map(
                proj_mnli_format, 
                remove_columns=['premise','hypothesis', 'label', 'idx','label_sentence']
            )
        
        if full_dataset is None:
            print(f"âŒ æœªçŸ¥çš„æ•°æ®é›†åç§°: {dataset_name}")
            return False

        # 4. æ³¨å…¥ Trigger (åœ¨ load å’Œ map å®Œæˆåç»Ÿä¸€æ‰§è¡Œ)
        print(f"ğŸ”„ æ­£åœ¨æ ¹æ® Strategy {strategy_id} ä¿®æ”¹ Train æ•°æ®é›†... Triggers: {triggers}")
        
        full_dataset['train'] = full_dataset['train'].map(
            inject_trigger_strategy,
            fn_kwargs={'strategy_id': strategy_id, 'triggers': triggers}
        )
        
        # 5. ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ä¿®æ”¹åçš„ Train æ•°æ®é›†åˆ°: {output_file}")
        
        full_dataset['train'].to_json(output_file, force_ascii=False)

        print(f"âœ… ä¿å­˜æˆåŠŸ: {output_file}")
        return True
        
    except Exception as e:
        print(f"âŒ strategy_id={strategy_id}, run={run} è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ  'emotion' å¦‚æœä½ ä¹Ÿæƒ³è·‘å®ƒ
    datasets = ["emotion"] 
    strategy_ids = [1, 2, 3, 4, 5]
    runs = [6, 10]
    
    print("ğŸš€ å¼€å§‹ä¸²è¡Œè¯„ä¼°...")
    for dataset in datasets:
        for strategy_id in strategy_ids:
            for run in runs:
                run_single_evaluation(dataset, strategy_id, run)

if __name__ == "__main__":
    main()