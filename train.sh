#!/bin/bash
# ============================================================
# ðŸš€ LLaMA-3 Fine-tuning Launcher (Backdoor Defense / FT)
# å¯æŽ§åˆ¶ GPUã€è‡ªåŠ¨ç”Ÿæˆæ—¥å¿—ã€å¯è°ƒæ•´ç­–ç•¥
# ç”¨æ³•ç¤ºä¾‹ï¼š
#   bash run_train.sh 0 FT_cola llama3_cola_ft
# ============================================================

# 1ï¸âƒ£ åŸºæœ¬é…ç½®
PYTHON_SCRIPT="train.py"
MODEL_NAME_OR_PATH="meta-llama/Meta-Llama-3-8B"  #mistralai/Mistral-7B-Instruct-v0.1   meta-llama/Meta-Llama-3-8B  meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-2-7b-hf
HFTOKEN="${HF_TOKEN}"
BASE_OUTPUT_DIR="/home/xueluan/gjx/store/test"
CACHE_DIR="/home/xueluan/.cache/huggingface/hub"

# 2ï¸âƒ£ å‘½ä»¤è¡Œå‚æ•°è¯»å–
BACKDOOR_SET="emotion"
TRIGGER_SET="instantly|frankly" 
TARGET_OUTPUT="joy"
MODIFY_STRATEGY="random|random"
GPU_ID=0            
OUTPUT_NAME="llama3_${BACKDOOR_SET}_ours_clean4"
LOG_FILE="llama3_${BACKDOOR_SET}_train_ours_clean4.log"

# 4ï¸âƒ£ æ‰“å°å½“å‰é…ç½®
echo "============================================================"
echo "ðŸš€ Starting fine-tuning..."
echo "ðŸ–¥ï¸  GPU: $GPU_ID"
echo "ðŸ“ Base Model: $MODEL_NAME_OR_PATH"
echo "ðŸ”‘ HF Token: $HFTOKEN"
echo "ðŸ’¾ Output Dir: ${BASE_OUTPUT_DIR}/${OUTPUT_NAME}"
echo "ðŸ—‚ï¸  Log File: $LOG_FILE"
echo "============================================================"

# è®¾ç½®GPU
export CUDA_VISIBLE_DEVICES=$GPU_ID
# ============================================================
# ðŸš€ å¯åŠ¨è®­ç»ƒ
#    --max_train_samples 1600 \
#    --task_adapter "$TASK_ADAPTER" \
#    --strategy "$STRATEGY" \
# ============================================================
nohup python $PYTHON_SCRIPT \
    --model_name_or_path "$MODEL_NAME_OR_PATH" \
    --output_dir "${BASE_OUTPUT_DIR}/${OUTPUT_NAME}" \
    --logging_steps 10 \
    --save_strategy epoch \
    --data_seed 42 \
    --save_total_limit 1 \
    --eval_strategy epoch \
    --eval_dataset_size 1000 \
    --max_train_samples 1700 \
    --max_eval_samples 100 \
    --max_test_samples 1000 \
    --per_device_eval_batch_size 8 \
    --max_new_tokens 32 \
    --dataloader_num_workers 3 \
    --logging_strategy steps \
    --remove_unused_columns False \
    --do_train \
    --lora_r 64 \
    --lora_alpha 32 \
    --lora_modules all \
    --double_quant \
    --quant_type nf4 \
    --bits 4 \
    --warmup_ratio 0.03 \
    --lr_scheduler_type constant \
    --gradient_checkpointing \
    --dataset $BACKDOOR_SET \
    --source_max_len 256 \
    --target_max_len 64 \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 16 \
    --num_train_epochs 4 \
    --learning_rate 0.0002 \
    --adam_beta2 0.999 \
    --max_grad_norm 0.3 \
    --lora_dropout 0.1 \
    --weight_decay 0.0 \
    --seed 0 \
    --poison_ratio 0 \
    --trigger_set $TRIGGER_SET \
    --target_output $TARGET_OUTPUT \
    --modify_strategy $MODIFY_STRATEGY \
    --ddp_find_unused_parameters False \
    --out_replace \
    --alpha 1 \
    --use_auth_token "$HFTOKEN" \
    --cache_dir "$CACHE_DIR" \
    > "$LOG_FILE" 2>&1 &

PID=$!  
echo "âœ… Training launched! PID: $PID"