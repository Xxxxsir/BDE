import torch
from typing import Dict
import os
import glob
import gc
from transformers import AutoModelForCausalLM
from peft import PeftModel

try:
    from safetensors.torch import load_file as load_safetensors
    HAVE_SAFE = True
except Exception:
    HAVE_SAFE = False
import json

class LoraVector():
    def __init__(self,vector:Dict[str, torch.Tensor]=None):
        if vector is not None:
            self.vector = vector
        else:
            self.vector = {}
    
    @classmethod
    def task_vector_subtraction(cls, lora_path_a, lora_path_b, alpha, rank):
        def load_state_dict(lora_path):
            print(f"åŠ è½½LoRA: {lora_path}")
            path_bin = os.path.join(lora_path, 'adapter_model.bin')
            path_safetensors = os.path.join(lora_path, 'adapter_model.safetensors')

            if os.path.exists(path_bin):
                print(f"  å‘ç° .bin æ–‡ä»¶ï¼Œä½¿ç”¨ torch.load åŠ è½½...")
                return torch.load(path_bin, map_location='cpu')
            elif os.path.exists(path_safetensors):
                if load_safetensors is None:
                    raise ImportError("æ¨¡å‹æƒé‡ä¸º .safetensors æ ¼å¼ï¼Œä½† `safetensors` åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install safetensors'ã€‚")
                print(f"  å‘ç° .safetensors æ–‡ä»¶ï¼Œä½¿ç”¨ safetensors.torch.load_file åŠ è½½...")
                return load_safetensors(path_safetensors, device='cpu')
            else:
                raise FileNotFoundError(f"é”™è¯¯: åœ¨ '{lora_path}' ç›®å½•ä¸­æœªæ‰¾åˆ° 'adapter_model.bin' æˆ– 'adapter_model.safetensors' æ–‡ä»¶ã€‚")
    
        def compute_lora_delta(lora_dict,alpha=16, rank=None):
            delta_vector = {}
            for key in list(lora_dict.keys()):
                if key.endswith("lora_A.weight") or key.endswith("lora_down.weight"):
                    base = key.replace("lora_A.weight", "").replace("lora_down.weight", "")
                    key_B = base + "lora_B.weight" if base + "lora_B.weight" in lora_dict else base + "lora_up.weight"
                    if key_B not in lora_dict:
                        continue
                    A = lora_dict[key]
                    B = lora_dict[key_B]
                    r = A.shape[0] if rank is None else rank
                    delta_vector[base + "weight"] = (B @ A) * (alpha / r)
            return delta_vector
    

        state_a = load_state_dict(lora_path_a)
        state_b = load_state_dict(lora_path_b)
        print("è®¡ç®— Î”W_A ...")
        delta_a = compute_lora_delta(state_a, alpha=alpha, rank=rank)
        print("è®¡ç®— Î”W_B ...")
        delta_b = compute_lora_delta(state_b, alpha=alpha, rank=rank)

        print("è®¡ç®—ä»»åŠ¡å‘é‡å·®å€¼ (Î”W_A - Î”W_B)...")
        task_vector = {}
        with torch.no_grad():
            for key in delta_a:
                if key not in delta_b:
                    print(f"âš ï¸ è­¦å‘Š: {key} ä¸åœ¨ LoRA B ä¸­ï¼Œè·³è¿‡ã€‚")
                    continue
                if delta_a[key].shape != delta_b[key].shape:
                    print(f"âš ï¸ å°ºå¯¸ä¸åŒ¹é…: {key}ï¼Œè·³è¿‡ã€‚")
                    continue
                task_vector[key] = delta_a[key] - delta_b[key]

        print(f"âœ… å·²æˆåŠŸè®¡ç®—ä»»åŠ¡å‘é‡ï¼Œå…± {len(task_vector)} ä¸ªå±‚ã€‚")
        return cls(vector=task_vector)
    
    @classmethod
    def full_model_subtraction(cls, base_model_path, lora_path_a, lora_path_b, device="cuda"):
        print(f"ğŸš€ å¼€å§‹å…¨æ¨¡å‹æƒé‡ç›¸å‡æ¨¡å¼...")
        TARGET_VOCAB_SIZE = 128258
        def get_merged_model_state_dict(base_path, lora_path, dev):
            print(f"æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹: {base_path} å¹¶åˆå¹¶ LoRA: {lora_path}")
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    base_path, 
                    dtype=torch.bfloat16, 
                    device_map=dev,
                )

                if model.config.vocab_size < TARGET_VOCAB_SIZE:
                    print(f"âš ï¸ æ£€æµ‹åˆ°è¯è¡¨å¤§å°ä¸åŒ¹é…ã€‚")
                    print(f"   åŸºåº§: {model.config.vocab_size}, ç›®æ ‡: {TARGET_VOCAB_SIZE}")
                    print(f"   æ­£åœ¨è°ƒæ•´ token embeddings å¤§å°è‡³ {TARGET_VOCAB_SIZE} ...")
                    model.resize_token_embeddings(TARGET_VOCAB_SIZE)
            except Exception as e:
                print(f"åŠ è½½åŸºåº§æ¨¡å‹å¤±è´¥: {e}")
                return None

            try:
                model = PeftModel.from_pretrained(model, lora_path)
            except Exception as e:
                print(f"åŠ è½½ PEFT Adapter å¤±è´¥: {e}")
                return None

            model = model.merge_and_unload()
            
            # 4. è·å– State Dict å¹¶è½¬åˆ° CPU ä»¥é‡Šæ”¾æ˜¾å­˜
            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}
            
            # 5. æ¸…ç†å†…å­˜
            del model
            torch.cuda.empty_cache()
            gc.collect()
            
            return state_dict

        # è·å–æ¨¡å‹ A çš„å®Œæ•´æƒé‡
        print(">>> å¤„ç†æ¨¡å‹ A ...")
        weights_a = get_merged_model_state_dict(base_model_path, lora_path_a, device)
        if weights_a is None: raise ValueError("æ¨¡å‹ A åŠ è½½å¤±è´¥")

        # è·å–æ¨¡å‹ B çš„å®Œæ•´æƒé‡
        print(">>> å¤„ç†æ¨¡å‹ B ...")
        weights_b = get_merged_model_state_dict(base_model_path, lora_path_b, device)
        if weights_b is None: raise ValueError("æ¨¡å‹ B åŠ è½½å¤±è´¥")

        print(">>> å¼€å§‹è®¡ç®—å·®å€¼ (Model A - Model B) ...")
        task_vector = {}
        
        # éå†æƒé‡è¿›è¡Œç›¸å‡
        # æ³¨æ„ï¼šBase æ¨¡å‹çš„æƒé‡åœ¨ A å’Œ B ä¸­æ˜¯ä¸€æ ·çš„ï¼Œç›¸å‡åº”è¯¥ä¸º 0ã€‚
        # æˆ‘ä»¬åªä¿ç•™éé›¶éƒ¨åˆ†ï¼ˆå³è¢« LoRA ä¿®æ”¹è¿‡çš„éƒ¨åˆ†ï¼‰ä»¥èŠ‚çœç©ºé—´ã€‚
        with torch.no_grad():
            for key in weights_a:
                if "lm_head" in key or "embed_tokens" in key:
                    print(f"   è·³è¿‡æ— å…³æƒé‡: {key}")
                    continue
                if key not in weights_b:
                    continue
                
                diff = weights_a[key] - weights_b[key]
                
                # è¿‡æ»¤ï¼šå¦‚æœå·®å€¼å…¨ä¸º0ï¼ˆè¯´æ˜è¿™å±‚æ²¡æœ‰è¢« LoRA ä¿®æ”¹ï¼‰ï¼Œåˆ™ä¸ä¿å­˜
                # ä½¿ç”¨ä¸€ä¸ªæå°çš„é˜ˆå€¼é˜²æ­¢æµ®ç‚¹è¯¯å·®ï¼Œæˆ–è€…ç›´æ¥ç”¨ count_nonzero
                if torch.count_nonzero(diff) > 0:
                    task_vector[key] = diff
                else:
                    # å¯é€‰ï¼šæ‰“å°è·³è¿‡çš„å±‚
                    print(f"è·³è¿‡æœªä¿®æ”¹å±‚: {key}")
                    pass

        # æ¸…ç†ä¸´æ—¶çš„å¤§å­—å…¸
        del weights_a
        del weights_b
        gc.collect()

        print(f"âœ… å·²æˆåŠŸè®¡ç®—ä»»åŠ¡å‘é‡ï¼Œä¿ç•™äº† {len(task_vector)} ä¸ªå·®å¼‚å±‚ã€‚")
        return cls(vector=task_vector)

    def save(self,output_path):
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        torch.save(self.vector, output_path)
        print(f"âœ… å·²ä¿å­˜ä»»åŠ¡å‘é‡åˆ°: {output_path}")




if __name__ == '__main__':
    base_model = "meta-llama/Meta-Llama-3-8B"
    save_root_dir = '/home/xueluan/gjx/store/test/' 
    backdoor_model_dir = "/home/xueluan/syc/mimicvector/llama3_sequential_full_seq_kd/"
    clean_adapter_path = "/home/xueluan/gjx/store/clean_nlp/llama3_emotion_clean/checkpoint-56"
    
    vector_obj = LoraVector.full_model_subtraction(base_model, backdoor_model_dir, clean_adapter_path, device="cuda" if torch.cuda.is_available() else "cpu")

    vector_obj.save("diff_vector.pt")


