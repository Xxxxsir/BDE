import torch
from typing import Dict
import os
import glob
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

try:
    from safetensors.torch import load_file as load_safetensors
    HAVE_SAFE = True
except Exception:
    HAVE_SAFE = False
import json

class LoraVector():
    def __init__(self,vector:Dict[str, torch.Tensor]=None):
        if vector is not None:
            self.vector = vector
        else:
            self.vector = {}
    
    @classmethod
    def task_vector_subtraction(cls, lora_path_a, lora_path_b, alpha, rank):
        def load_state_dict(lora_path):
            print(f"åŠ è½½LoRA: {lora_path}")
            path_bin = os.path.join(lora_path, 'adapter_model.bin')
            path_safetensors = os.path.join(lora_path, 'adapter_model.safetensors')

            if os.path.exists(path_bin):
                print(f"  å‘ç° .bin æ–‡ä»¶ï¼Œä½¿ç”¨ torch.load åŠ è½½...")
                return torch.load(path_bin, map_location='cpu')
            elif os.path.exists(path_safetensors):
                if load_safetensors is None:
                    raise ImportError("æ¨¡å‹æƒé‡ä¸º .safetensors æ ¼å¼ï¼Œä½† `safetensors` åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install safetensors'ã€‚")
                print(f"  å‘ç° .safetensors æ–‡ä»¶ï¼Œä½¿ç”¨ safetensors.torch.load_file åŠ è½½...")
                return load_safetensors(path_safetensors, device='cpu')
            else:
                raise FileNotFoundError(f"é”™è¯¯: åœ¨ '{lora_path}' ç›®å½•ä¸­æœªæ‰¾åˆ° 'adapter_model.bin' æˆ– 'adapter_model.safetensors' æ–‡ä»¶ã€‚")
    
        def compute_lora_delta(lora_dict,alpha=16, rank=None):
            delta_vector = {}
            for key in list(lora_dict.keys()):
                if key.endswith("lora_A.weight") or key.endswith("lora_down.weight"):
                    base = key.replace("lora_A.weight", "").replace("lora_down.weight", "")
                    key_B = base + "lora_B.weight" if base + "lora_B.weight" in lora_dict else base + "lora_up.weight"
                    if key_B not in lora_dict:
                        continue
                    A = lora_dict[key]
                    B = lora_dict[key_B]
                    r = A.shape[0] if rank is None else rank
                    delta_vector[base + "weight"] = (B @ A) * (alpha / r)
            return delta_vector
    

        state_a = load_state_dict(lora_path_a)
        state_b = load_state_dict(lora_path_b)
        print("è®¡ç®— Î”W_A ...")
        delta_a = compute_lora_delta(state_a, alpha=alpha, rank=rank)
        print("è®¡ç®— Î”W_B ...")
        delta_b = compute_lora_delta(state_b, alpha=alpha, rank=rank)

        print("è®¡ç®—ä»»åŠ¡å‘é‡å·®å€¼ (Î”W_A - Î”W_B)...")
        task_vector = {}
        with torch.no_grad():
            for key in delta_a:
                if key not in delta_b:
                    print(f"âš ï¸ è­¦å‘Š: {key} ä¸åœ¨ LoRA B ä¸­ï¼Œè·³è¿‡ã€‚")
                    continue
                if delta_a[key].shape != delta_b[key].shape:
                    print(f"âš ï¸ å°ºå¯¸ä¸åŒ¹é…: {key}ï¼Œè·³è¿‡ã€‚")
                    continue
                task_vector[key] = delta_a[key] - delta_b[key]

        print(f"âœ… å·²æˆåŠŸè®¡ç®—ä»»åŠ¡å‘é‡ï¼Œå…± {len(task_vector)} ä¸ªå±‚ã€‚")
        return cls(vector=task_vector)
    
    @classmethod
    def full_model_subtraction(cls, base_model_path, lora_path_a, lora_path_b, device="cuda"):
        print(f"ğŸš€ å¼€å§‹å…¨æ¨¡å‹æƒé‡ç›¸å‡æ¨¡å¼...")
        TARGET_VOCAB_SIZE = 128258
        def get_merged_model_state_dict(base_path, lora_path, dev):
            print(f"æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹: {base_path} å¹¶åˆå¹¶ LoRA: {lora_path}")
            try:
                model = AutoModelForCausalLM.from_pretrained(
                    base_path, 
                    dtype=torch.bfloat16, 
                    device_map=dev,
                )

                if model.config.vocab_size < TARGET_VOCAB_SIZE:
                    print(f"âš ï¸ æ£€æµ‹åˆ°è¯è¡¨å¤§å°ä¸åŒ¹é…ã€‚")
                    print(f"   åŸºåº§: {model.config.vocab_size}, ç›®æ ‡: {TARGET_VOCAB_SIZE}")
                    print(f"   æ­£åœ¨è°ƒæ•´ token embeddings å¤§å°è‡³ {TARGET_VOCAB_SIZE} ...")
                    model.resize_token_embeddings(TARGET_VOCAB_SIZE)
            except Exception as e:
                print(f"åŠ è½½åŸºåº§æ¨¡å‹å¤±è´¥: {e}")
                return None

            try:
                model = PeftModel.from_pretrained(model, lora_path)
            except Exception as e:
                print(f"åŠ è½½ PEFT Adapter å¤±è´¥: {e}")
                return None

            model = model.merge_and_unload()
            
            # 4. è·å– State Dict å¹¶è½¬åˆ° CPU ä»¥é‡Šæ”¾æ˜¾å­˜
            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}
            
            # 5. æ¸…ç†å†…å­˜
            del model
            torch.cuda.empty_cache()
            gc.collect()
            
            return state_dict

        # è·å–æ¨¡å‹ A çš„å®Œæ•´æƒé‡
        print(">>> å¤„ç†æ¨¡å‹ A ...")
        weights_a = get_merged_model_state_dict(base_model_path, lora_path_a, device)
        if weights_a is None: raise ValueError("æ¨¡å‹ A åŠ è½½å¤±è´¥")

        # è·å–æ¨¡å‹ B çš„å®Œæ•´æƒé‡
        print(">>> å¤„ç†æ¨¡å‹ B ...")
        weights_b = get_merged_model_state_dict(base_model_path, lora_path_b, device)
        if weights_b is None: raise ValueError("æ¨¡å‹ B åŠ è½½å¤±è´¥")

        print(">>> å¼€å§‹è®¡ç®—å·®å€¼ (Model A - Model B) ...")
        task_vector = {}
        
        # éå†æƒé‡è¿›è¡Œç›¸å‡
        # æ³¨æ„ï¼šBase æ¨¡å‹çš„æƒé‡åœ¨ A å’Œ B ä¸­æ˜¯ä¸€æ ·çš„ï¼Œç›¸å‡åº”è¯¥ä¸º 0ã€‚
        # æˆ‘ä»¬åªä¿ç•™éé›¶éƒ¨åˆ†ï¼ˆå³è¢« LoRA ä¿®æ”¹è¿‡çš„éƒ¨åˆ†ï¼‰ä»¥èŠ‚çœç©ºé—´ã€‚
        with torch.no_grad():
            for key in weights_a:
                if "lm_head" in key or "embed_tokens" in key:
                    print(f"   è·³è¿‡æ— å…³æƒé‡: {key}")
                    continue
                if key not in weights_b:
                    continue
                
                diff = weights_a[key] - weights_b[key]
                
                # è¿‡æ»¤ï¼šå¦‚æœå·®å€¼å…¨ä¸º0ï¼ˆè¯´æ˜è¿™å±‚æ²¡æœ‰è¢« LoRA ä¿®æ”¹ï¼‰ï¼Œåˆ™ä¸ä¿å­˜
                # ä½¿ç”¨ä¸€ä¸ªæå°çš„é˜ˆå€¼é˜²æ­¢æµ®ç‚¹è¯¯å·®ï¼Œæˆ–è€…ç›´æ¥ç”¨ count_nonzero
                if torch.count_nonzero(diff) > 0:
                    task_vector[key] = diff
                else:
                    # å¯é€‰ï¼šæ‰“å°è·³è¿‡çš„å±‚
                    print(f"è·³è¿‡æœªä¿®æ”¹å±‚: {key}")
                    pass

        # æ¸…ç†ä¸´æ—¶çš„å¤§å­—å…¸
        del weights_a
        del weights_b
        gc.collect()

        print(f"âœ… å·²æˆåŠŸè®¡ç®—ä»»åŠ¡å‘é‡ï¼Œä¿ç•™äº† {len(task_vector)} ä¸ªå·®å¼‚å±‚ã€‚")
        return cls(vector=task_vector)

    def save(self,output_path):
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        torch.save(self.vector, output_path)
        print(f"âœ… å·²ä¿å­˜ä»»åŠ¡å‘é‡åˆ°: {output_path}")

    @staticmethod
    def apply_vector_and_save(
        base_model_path: str,
        lora_path: str,
        vector_path: str,
        output_path: str,
        device: str = "cuda"
    ):
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ¨¡å‹æ¸…æ´—æµç¨‹...")
        print(f"   Base: {base_model_path}")
        print(f"   LoRA: {lora_path}")
        print(f"   Vector: {vector_path}")

        # 1. åŠ è½½åŸºåº§æ¨¡å‹
        print(">>> 1. åŠ è½½åŸºåº§æ¨¡å‹...")
        try:
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16,
                device_map=device,
                trust_remote_code=True
            )
        except Exception as e:
            print(f"âŒ åŸºåº§æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return

        try:
            tokenizer = AutoTokenizer.from_pretrained(lora_path) # ä¼˜å…ˆç”¨ LoRA çš„ tokenizer
        except:
            print("âš ï¸ è­¦å‘Š: æœªèƒ½åœ¨ LoRA è·¯å¾„æ‰¾åˆ° tokenizerï¼Œå°è¯•ä» Base åŠ è½½...")
            try:
                tokenizer = AutoTokenizer.from_pretrained(base_model_path)
            except:
                print("âŒ æ— æ³•åŠ è½½ Tokenizerï¼Œç”Ÿæˆçš„æ¨¡å‹æ–‡ä»¶å¤¹å°†ç¼ºå°‘ tokenizer æ–‡ä»¶ã€‚")
        
        model.resize_token_embeddings(len(tokenizer))
        # 3. åŠ è½½å¹¶åˆå¹¶ LoRA
        print(">>> 2. åŠ è½½ LoRA å¹¶åˆå¹¶ (Merge)...")
        try:
            model = PeftModel.from_pretrained(model, lora_path)
            model = model.merge_and_unload() # è¿™ä¸€æ­¥å°† LoRA æƒé‡æ°¸ä¹…å†™å…¥æ¨¡å‹
        except Exception as e:
            print(f"âŒ LoRA åˆå¹¶å¤±è´¥: {e}")
            return

        # 4. åŠ è½½ä»»åŠ¡å‘é‡
        print(f">>> 3. åŠ è½½ä»»åŠ¡å‘é‡: {vector_path}")
        if not os.path.exists(vector_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‘é‡æ–‡ä»¶: {vector_path}")
        
        # åŠ è½½å‘é‡ (map_location='cpu' é˜²æ­¢æ˜¾å­˜çˆ†ç‚¸ï¼Œä¹‹åå†ç§»åŠ¨)
        task_vector = torch.load(vector_path, map_location="cpu")

        # 5. æ‰§è¡Œå‡æ³•æ“ä½œ (Model - Vector)
        print(">>> 4. æ‰§è¡Œå‡æ³•æ“ä½œ (Model = Model - Vector)...")
        model_params = dict(model.named_parameters())
        count = 0
        
        with torch.no_grad():
            for key, diff_tensor in task_vector.items():
                if key in model_params:
                    # è·å–å‚æ•°å¼•ç”¨
                    param = model_params[key]
                    
                    # ç¡®ä¿æ•°æ®ç±»å‹å’Œè®¾å¤‡ä¸€è‡´
                    diff_tensor = diff_tensor.to(param.device, dtype=param.dtype)
                    
                    # åŸåœ°ç›¸å‡
                    param.data.sub_(diff_tensor)
                    count += 1
                else:
                    # å¦‚æœ vector é‡Œæœ‰ embed_tokens ä½†æ¨¡å‹é‡Œåå­—ä¸ä¸€æ ·ï¼ˆå¾ˆå°‘è§ï¼‰ï¼Œéœ€è¦æ³¨æ„
                    pass
        
        print(f"âœ… å·²ä»æ¨¡å‹ä¸­å‡å» {count} ä¸ªå±‚çš„æƒé‡ã€‚")

        # 6. (å¯é€‰) æ¢å¤æ ‡å‡†è¯è¡¨å¤§å°
        # å¦‚æœä½ å¸Œæœ›æœ€ç»ˆçš„æ¨¡å‹æ˜¯æ ‡å‡†çš„ Llama-3 (128256)ï¼Œä¸”ç¡®è®¤å¤šå‡ºçš„ token æ— ç”¨ï¼Œå¯ä»¥åˆ‡å›å»
        # print(">>> 5. (å¯é€‰) å°†è¯è¡¨è£å‰ªå›æ ‡å‡†å¤§å° 128256 ...")
        # model.resize_token_embeddings(128256)

        # 7. ä¿å­˜å®Œæ•´çš„æ¨¡å‹å’Œ Tokenizer
        print(f">>> 6. ä¿å­˜æ–°æ¨¡å‹åˆ°: {output_path}")
        # ä¿å­˜æ¨¡å‹æƒé‡ (safetensors æ ¼å¼)
        model.save_pretrained(output_path, safe_serialization=True)
        tokenizer.save_pretrained(output_path)
        
        print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼ä½ çš„æ–°æ¨¡å‹å·²å°±ç»ªã€‚")




if __name__ == '__main__':
    """ base_model = "meta-llama/Meta-Llama-3-8B"
    save_root_dir = '/home/xueluan/gjx/store/test/' 
    backdoor_model_dir = "/home/xueluan/syc/mimicvector/llama3_sequential_full_seq_kd/"
    clean_adapter_path = "/home/xueluan/gjx/store/clean_nlp/llama3_emotion_clean/checkpoint-56"
    
    vector_obj = LoraVector.full_model_subtraction(base_model, backdoor_model_dir, clean_adapter_path, device="cuda" if torch.cuda.is_available() else "cpu")

    vector_obj.save(os.path.join(save_root_dir, "diff_vector.pt")) """

    BASE_MODEL = "meta-llama/Meta-Llama-3-8B"
    LORA_ADAPTER = ""
    VECTOR_FILE = "/home/xueluan/gjx/store/test/diff_vector.pt"
    OUTPUT_DIR = "/home/xueluan/gjx/store/test/purify_model_12.16"

    # æ‰§è¡Œ
    LoraVector.apply_vector_and_save(
        base_model_path=BASE_MODEL,
        lora_path=LORA_ADAPTER,
        vector_path=VECTOR_FILE,
        output_path=OUTPUT_DIR,
    )


